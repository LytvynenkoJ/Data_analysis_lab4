{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7717e59",
   "metadata": {},
   "source": [
    "P.S. Оскільки виникли деякі технічні проблеми, довелось скористатись стареньким ноубуком, а йому було дуже важко і навіть після того як я залишила таку доволі куценьку модель воно дуже довго працювало. Можливо, якщо я полікую новенький ноутбук, то зроблю перекладач з кращою точністю, але і цей в принципі щось перекладає)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbed5fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "#tf.config.experimental_run_functions_eagerly(False)\n",
    "#from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "#import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6702e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.layers import TextVectorization, LeakyReLU, Conv2DTranspose, Embedding, Bidirectional, Reshape, Dense, BatchNormalization, LSTM\n",
    "from keras.layers import GRU, Conv2D, MaxPooling2D, Flatten, Dropout, MultiHeadAttention, LayerNormalization, Add, StringLookup\n",
    "from keras import Sequential\n",
    "from keras.models import save_model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f62903ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_text as tf_text\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd46712",
   "metadata": {},
   "source": [
    "## Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b30a8",
   "metadata": {},
   "source": [
    "Переклад: Англійська - Французька"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fbe01ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"fra-eng.txt\", 'r', encoding='utf-8')\n",
    "text = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8678f45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub('\\s{2,}', \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "107f1eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = text.split(\"\\n\")[:-1]\n",
    "eng = []\n",
    "fra = []\n",
    "for l in lines:\n",
    "    eng.append(cleaning(l.split(\"\\t\")[0]))\n",
    "    fra.append(cleaning(l.split(\"\\t\")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdf3df3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i left  cours \n"
     ]
    }
   ],
   "source": [
    "print(eng[111], fra[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de5fb45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = string.punctuation\n",
    "punct.replace('[', '')\n",
    "punct.replace(']', '')\n",
    "\n",
    "def standardization(text):\n",
    "    text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text, \"[%s]\" % re.escape(punct), \"\")\n",
    "    return tf.strings.join(['[START]', text, '[END]'], separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bb357d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000\n",
    "sequence_length = 100\n",
    "batch_size = 64\n",
    "buffer_size = len(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e10d3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vectorization = TextVectorization(max_tokens = num_words, output_mode=\"int\",\n",
    "                                      standardize=standardization #output_sequence_length=sequence_length\n",
    "                                     )\n",
    "fra_vectorization = TextVectorization(max_tokens = num_words, output_mode=\"int\",\n",
    "                                      standardize=standardization #output_sequence_length=sequence_length\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d9745ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vectorization.adapt(eng)\n",
    "fra_vectorization.adapt(fra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a90d24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_fra = np.array(fra_vectorization.get_vocabulary())\n",
    "vocab_eng = np.array(eng_vectorization.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ecfa63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(eng, fra, train_size=0.7)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc4d903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(eng, fra):\n",
    "    eng = eng_vectorization(eng)\n",
    "    fra = fra_vectorization(fra)\n",
    "    return (eng, fra[:,:-1]),fra[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7ef20b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(eng, fra):\n",
    "    eng_texts = list(eng)\n",
    "    spa_texts = list(fra)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(vectorize_text)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74a7d2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = make_dataset(x_train, y_train)\n",
    "test_data = make_dataset(x_test, y_test)\n",
    "val_data = make_dataset(x_val,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869e3f87",
   "metadata": {},
   "source": [
    "## Attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc4f2b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9319329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context):\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=context,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "\n",
    "        self.last_attn_scores = attn_scores\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d831b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de343cdf",
   "metadata": {},
   "source": [
    "## The encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73f8dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads,\n",
    "               dff, vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, d_model)\n",
    "\n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "        \n",
    "        self.seq = Sequential([Dense(dff, activation='relu')])\n",
    "\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        \n",
    "        #self.dropout = Dropout(dropout_rate)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.self_attention(x)\n",
    "        x = self.seq(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d6a8ab",
   "metadata": {},
   "source": [
    "## The decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a60af277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, vocab_size,\n",
    "               dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, d_model)\n",
    "        \n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "\n",
    "        self.cross_attention = CrossAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.seq = Sequential([Dense(dff, activation='relu')])\n",
    "\n",
    "        self.layer_norm = LayerNormalization()\n",
    "\n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x, context):\n",
    "        x = self.embedding(x)  \n",
    "\n",
    "        x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "        self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "        x = self.seq(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ce355e",
   "metadata": {},
   "source": [
    "## Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5de9cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 128\n",
    "dff = 128\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "embed_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c3ddf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_acc(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "\n",
    "    match = tf.cast(y_true == y_pred, tf.float32)\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07ea3b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(y_true, y_pred):\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(y_true != 0, loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a69032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "#x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = Encoder(d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=eng_vectorization.vocabulary_size(),\n",
    "                           dropout_rate=dropout_rate)(encoder_inputs)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d563b485",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "#x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = Decoder(d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=fra_vectorization.vocabulary_size(),\n",
    "                           dropout_rate=dropout_rate)(decoder_inputs, encoded_seq_inputs)\n",
    "#x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(fra_vectorization.vocabulary_size(), activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69fe27c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cee1646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(optimizer='adam',\n",
    "              loss=masked_loss, \n",
    "              metrics=[masked_acc, masked_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ceddc9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " encoder (Encoder)              (None, None, 128)    1296512     ['encoder_inputs[0][0]']         \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, None, 10000)  3114256     ['decoder_inputs[0][0]',         \n",
      "                                                                  'encoder[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,410,768\n",
      "Trainable params: 4,410,768\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f1728a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\keras\\backend.py:5582: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1852/1852 [==============================] - 14083s 8s/step - loss: 3.6659 - masked_acc: 0.3625 - masked_loss: 3.6655 - val_loss: 2.7151 - val_masked_acc: 0.4687 - val_masked_loss: 2.7153\n",
      "Epoch 2/20\n",
      "1852/1852 [==============================] - 14890s 8s/step - loss: 2.2774 - masked_acc: 0.5139 - masked_loss: 2.2773 - val_loss: 2.2641 - val_masked_acc: 0.5279 - val_masked_loss: 2.2650\n",
      "Epoch 3/20\n",
      "1852/1852 [==============================] - 14869s 8s/step - loss: 1.8418 - masked_acc: 0.5660 - masked_loss: 1.8416 - val_loss: 2.1545 - val_masked_acc: 0.5479 - val_masked_loss: 2.1551\n",
      "Epoch 4/20\n",
      "1852/1852 [==============================] - 15229s 8s/step - loss: 1.6226 - masked_acc: 0.5986 - masked_loss: 1.6224 - val_loss: 2.1335 - val_masked_acc: 0.5584 - val_masked_loss: 2.1338\n",
      "Epoch 5/20\n",
      "1852/1852 [==============================] - 14629s 8s/step - loss: 1.4840 - masked_acc: 0.6226 - masked_loss: 1.4838 - val_loss: 2.1444 - val_masked_acc: 0.5644 - val_masked_loss: 2.1447\n",
      "Epoch 6/20\n",
      "1852/1852 [==============================] - 10104s 5s/step - loss: 1.3838 - masked_acc: 0.6415 - masked_loss: 1.3836 - val_loss: 2.1592 - val_masked_acc: 0.5687 - val_masked_loss: 2.1595\n",
      "Epoch 7/20\n",
      "1852/1852 [==============================] - 7347s 4s/step - loss: 1.3058 - masked_acc: 0.6575 - masked_loss: 1.3056 - val_loss: 2.1872 - val_masked_acc: 0.5715 - val_masked_loss: 2.1875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24c258e1b80>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.fit(train_data, epochs=20, validation_data=val_data, callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad6f2c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "618/618 [==============================] - 1127s 2s/step - loss: 2.1913 - masked_acc: 0.5695 - masked_loss: 2.1916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.1913132667541504, 0.5695332884788513, 2.1915767192840576]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1271a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "fra_vocab = fra_vectorization.get_vocabulary()\n",
    "fra_index_lookup = dict(zip(range(len(fra_vocab)), fra_vocab))\n",
    "max_decoded_sentence_length = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2ab5abce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[START]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = fra_vectorization([decoded_sentence])[:, :-1]\n",
    "        \n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "        \n",
    "        \n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = fra_index_lookup[sampled_token_index]\n",
    "\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[END]\" or sampled_token == \"[UNK]\":\n",
    "            break\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "656eabca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_translation(sentence, translated_text, ground_truth):\n",
    "    print(\"eng sentence:             \" + sentence)\n",
    "    print(\"translated fra sentence:  \" + translated_text)\n",
    "    print(\"actually fra sentence:    \" + ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9ec584bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng sentence:             tom was all worn out\n",
      "translated fra sentence:  [START] tom tout était ce arrangé que [END]\n",
      "actually fra sentence:    tom était tout usé\n"
     ]
    }
   ],
   "source": [
    "sentence = 'tom was all worn out'\n",
    "ground_truth = 'tom était tout usé'\n",
    "\n",
    "translated_text = decode_sequence(sentence)\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "90cd7341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng sentence:             they ve washed their hands\n",
      "translated fra sentence:  [START] ils leurs ont mains [UNK]\n",
      "actually fra sentence:    ils se sont lavé les mains\n"
     ]
    }
   ],
   "source": [
    "sentence = 'they ve washed their hands'\n",
    "ground_truth = 'ils se sont lavé les mains'\n",
    "\n",
    "translated_text = decode_sequence(sentence)\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6d7ed5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng sentence:             let s make a trade\n",
      "translated fra sentence:  [START] faisons secrète un [END]\n",
      "actually fra sentence:    faisons un échange\n"
     ]
    }
   ],
   "source": [
    "sentence = 'let s make a trade'\n",
    "ground_truth = 'faisons un échange'\n",
    "\n",
    "translated_text = decode_sequence(sentence)\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cf90f92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng sentence:             her condition got worse last night\n",
      "translated fra sentence:  [START] sa la gueule nuit de dernière sa [END]\n",
      "actually fra sentence:    son état s'est aggravé la nuit dernière\n"
     ]
    }
   ],
   "source": [
    "sentence = 'her condition got worse last night'\n",
    "ground_truth = 'son état s\\'est aggravé la nuit dernière'\n",
    "\n",
    "translated_text = decode_sequence(sentence)\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4aaf00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
